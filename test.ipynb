{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import datetime\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "# Imports\n",
    "\n",
    "\n",
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# args = {\"shape_predictor\":unicode() ,\n",
    "#         \"video\": 1000}\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "# predictor = dlib.shape_predictor(args[\"shape_predictor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((68, 2), dtype=dtype)\n",
    "\t# loop over the 68 facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, 68):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords\n",
    "    \n",
    "def rect_to_bb(rect):\n",
    "\t# take a bounding predicted by dlib and convert it\n",
    "\t# to the format (x, y, w, h) as we would normally do\n",
    "\t# with OpenCV\n",
    "\tx = rect.left()\n",
    "\ty = rect.top()\n",
    "\tw = rect.right() - x\n",
    "\th = rect.bottom() - y\n",
    "\t# return a tuple of (x, y, w, h)\n",
    "\treturn (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid = cv2.VideoCapture(0) \n",
    "# font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# while(True): \n",
    "      \n",
    "#     # Capture the video frame \n",
    "#     # by frame \n",
    "#     ret, frame = vid.read()    \n",
    "#     frame = imutils.resize(frame, width=400)\n",
    "#     if(type(frame)!=None):\n",
    "#         img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         if(img.any()):\n",
    "#             rects = dlib.get_frontal_face_detector(img,0)\n",
    "#             for (i, rect) in enumerate(rects):\n",
    "#                 # determine the facial landmarks for the face region, then\n",
    "#                 # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "#                 # array\n",
    "#                 shape = dlib.shape_predictor(img,rect)\n",
    "#                 shape = face_utils.shape_to_np(shape)\n",
    "#                 # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "#                 # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "#                 (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "#                 cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#                 # show the face number\n",
    "#                 cv2.putText(img, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "#                 # loop over the (x, y)-coordinates for the facial landmarks\n",
    "#                 # and draw them on the img\n",
    "#                 for (x, y) in shape:\n",
    "#                     cv2.circle(img, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "#     # Display the resulting frame \n",
    "#     cv2.imshow('frame', img) \n",
    "      \n",
    "#     # the 'q' button is set as the \n",
    "#     # quitting button you may use any \n",
    "#     # desired button of your choice \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "#         break\n",
    "  \n",
    "# # After the loop release the cap object \n",
    "# vid.release() \n",
    "# # Destroy all the windows \n",
    "# cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-3-9afa914336a6>, line 20)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-9afa914336a6>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    cv2.imshow(\"Test\", frame)\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] camera sensor warming up...\")\n",
    "vs = VideoStream().start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    if(type(frame)!=type(None)):\n",
    "        # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # rects =  dlib.get_frontal_face_detector(shape_to_np(gray), 0)\n",
    "        # for rect in rects:\n",
    "        #     shape = dlib.shape_predictor(gray, rect)\n",
    "        #     shape = shape_to_np(shape)\n",
    "        #     # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        #     # and draw them on the \n",
    "        #     for (x, y) in shape:\n",
    "        #         cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\t  \n",
    "\t# show the frame\n",
    "\tcv2.imshow(\"Test\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] camera sensor warming up...\n"
    }
   ],
   "source": [
    "print(\"[INFO] camera sensor warming up...\")\n",
    "vs = VideoStream().start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    if(type(frame)!=type(None)):\n",
    "        # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # rects =  dlib.get_frontal_face_detector(shape_to_np(gray), 0)\n",
    "        # for rect in rects:\n",
    "        #     shape = dlib.shape_predictor(gray, rect)\n",
    "        #     shape = shape_to_np(shape)\n",
    "        #     # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        #     # and draw them on the \n",
    "        #     for (x, y) in shape:\n",
    "        #         cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\t  \n",
    "\t# show the frame\n",
    "\tcv2.imshow(\"Test\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initializing Facial Landmarking Predictor ->\nOrderedDict([('mouth', (48, 68)), ('inner_mouth', (60, 68)), ('right_eyebrow', (17, 22)), ('left_eyebrow', (22, 27)), ('right_eye', (36, 42)), ('left_eye', (42, 48)), ('nose', (27, 36)), ('jaw', (0, 17))])\n"
    }
   ],
   "source": [
    "\n",
    " \n",
    "\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "\t# compute the euclidean distances between the two sets of\n",
    "\t# vertical eye landmarks (x, y)-coordinates\n",
    "\tA = dist.euclidean(eye[1], eye[5])\n",
    "\tB = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "\t# compute the euclidean distance between the horizontal\n",
    "\t# eye landmark (x, y)-coordinates\n",
    "\tC = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "\t# compute the eye aspect ratio\n",
    "\tear = (A + B) / (2.0 * C)\n",
    "\n",
    "\t# return the eye aspect ratio\n",
    "\treturn ear\n",
    " \n",
    "# Initializing dlib's face detector (HOG-based)\n",
    "# Creating the facial shape predictor using the 'shape_predictor_68_face_landmarks.dat' file\n",
    "print(\"Initializing Facial Landmarking Predictor ->\")\n",
    "#Detecting the frontal faces\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Initializing the camera sensor to warm up\n",
    "vs = VideoStream(0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "blink_count = 0\n",
    "total = 0\n",
    "\n",
    "# Mouth\n",
    "mouthDistance=0\n",
    "mouthCounter=0\n",
    "\n",
    "print(face_utils.FACIAL_LANDMARKS_IDXS)\n",
    "# Looping over all the frames from the webcam stream\n",
    "while True:\n",
    "    # Grabbing the frames\n",
    "\tframe = vs.read()\n",
    "\tif(type(frame)!=type(None)):\n",
    "\t\t# Setting the frame size to 500\n",
    "\t\tframe = imutils.resize(frame, width=500)\n",
    "\t\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\t# cv2.putText(frame, \"Blink Count = \" + str(total), (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\t\t# Detecting the faces from the frame\n",
    "\t\trects = detector(frame, 0)\n",
    "\t\t\n",
    "\n",
    "\t\t# Looping through each face detection\n",
    "\t\tfor rect in rects:\n",
    "\t\t\tshape = predictor(gray, rect)\n",
    "\t\t\tshape = face_utils.shape_to_np(shape)\n",
    "\n",
    "\t\t\tleftEye = shape[lStart:lEnd]\n",
    "\t\t\trightEye = shape[rStart:rEnd]\n",
    "\t\t\tleftEAR = eye_aspect_ratio(leftEye)\n",
    "\t\t\trightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "\n",
    "\t\t\t##     Mouth\n",
    "\t\t\tmouthDistance=shape[67]-shape[63]\n",
    "\t\t\tif(mouthDistance[1]>10):\n",
    "\t\t\t\tmouthCounter+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tmouthCounter=0\n",
    "\t\t\tif(mouthDistance[1]>10 and mouthCounter>20):\n",
    "\t\t\t\tcv2.putText(frame, \"Yawn\", (0, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\t\t\t# average the eye aspect ratio together for both eyes\n",
    "\t\t\tear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "\t\t\tleftEyeHull = cv2.convexHull(leftEye)\n",
    "\t\t\trightEyeHull = cv2.convexHull(rightEye)\n",
    "\t\t\t# cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\t\t# cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\t\t\n",
    "\t\t\tif ear < 0.25:\n",
    "\t\t\t\tblink_count += 1\n",
    "\t\n",
    "\t\t\t# otherwise, the eye aspect ratio is not below the blink\n",
    "\t\t\t# threshold\n",
    "\t\t\telse:\n",
    "\t\t\t\t# if the eyes were closed for a sufficient number of\n",
    "\t\t\t\t# then increment the total number of blinks\n",
    "\t\t\t\tif blink_count >= 3:\n",
    "\t\t\t\t\ttotal += 1\n",
    "\t\n",
    "\t\t\t\t# reset the eye frame counter\n",
    "\t\t\t\tblink_count = 0\n",
    "\t\t\t\t\n",
    "\t\t\t# Looping over the facial landmarks based on their (x,y) coordinates\n",
    "\t\t\t# Drawing a red circle over those landmarks\n",
    "\t\t\tfor (x, y) in shape:\n",
    "\t\t\t\tcv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t# Showing the frame\n",
    "\t\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "\t# Break from loop/end when the 'q' is pressed\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "        \n",
    "# Destroying the windows once the frame display is over\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initializing Facial Landmarking Predictor ->\nOrderedDict([('mouth', (48, 68)), ('inner_mouth', (60, 68)), ('right_eyebrow', (17, 22)), ('left_eyebrow', (22, 27)), ('right_eye', (36, 42)), ('left_eye', (42, 48)), ('nose', (27, 36)), ('jaw', (0, 17))])\n-0.0666666666667\n0.23200265492\n"
    }
   ],
   "source": [
    "###################### TEST PHASE ############################\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "\t# compute the euclidean distances between the two sets of\n",
    "\t# vertical eye landmarks (x, y)-coordinates\n",
    "\tA = dist.euclidean(eye[1], eye[5])\n",
    "\tB = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "\t# compute the euclidean distance between the horizontal\n",
    "\t# eye landmark (x, y)-coordinates\n",
    "\tC = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "\t# compute the eye aspect ratio\n",
    "\tear = (A + B) / (2.0 * C)\n",
    "\n",
    "\t# return the eye aspect ratio\n",
    "\treturn ear\n",
    " \n",
    "# Initializing dlib's face detector (HOG-based)\n",
    "# Creating the facial shape predictor using the 'shape_predictor_68_face_landmarks.dat' file\n",
    "print(\"Initializing Facial Landmarking Predictor ->\")\n",
    "#Detecting the frontal faces\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Initializing the camera sensor to warm up\n",
    "vs = VideoStream(0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "blink_count = 0\n",
    "total = 0\n",
    "\n",
    "# Mouth\n",
    "mouthDistance=[]\n",
    "\n",
    "# Eyes\n",
    "ears=[]\n",
    "\n",
    "counter=0\n",
    "\n",
    "print(face_utils.FACIAL_LANDMARKS_IDXS)\n",
    "# Looping over all the frames from the webcam stream\n",
    "while True:\n",
    "    # Grabbing the frames\n",
    "\tframe = vs.read()\n",
    "\tif(type(frame)!=type(None)):\n",
    "\t\t# Setting the frame size to 500\n",
    "\t\tframe = imutils.resize(frame, width=500)\n",
    "\t\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\tcv2.putText(frame, 'Stare at the camera', (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\t\t# Detecting the faces from the frame\n",
    "\t\trects = detector(frame, 0)\n",
    "\t\tcounter+=1\n",
    "\t\tif(counter>60):\n",
    "\t\t\tbreak\n",
    "\t\t# Looping through each face detection\n",
    "\t\tfor rect in rects:\n",
    "\t\t\tshape = predictor(gray, rect)\n",
    "\t\t\tshape = face_utils.shape_to_np(shape)\n",
    "\n",
    "\t\t\tleftEye = shape[lStart:lEnd]\n",
    "\t\t\trightEye = shape[rStart:rEnd]\n",
    "\t\t\tleftEAR = eye_aspect_ratio(leftEye)\n",
    "\t\t\trightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "\n",
    "\t\t\t##     Mouth\n",
    "\t\t\tdistanceOfMouth=shape[67]-shape[63]\n",
    "\t\t\tmouthDistance.append(distanceOfMouth[1])\n",
    "\n",
    "\n",
    "\t\t\t# average the eye aspect ratio together for both eyes\n",
    "\t\t\tear = (leftEAR + rightEAR) / 2.0\n",
    "\t\t\tears.append(ear)\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t# Looping over the facial landmarks based on their (x,y) coordinates\n",
    "\t\t\t# Drawing a red circle over those landmarks\n",
    "\t\t\tfor (x, y) in shape:\n",
    "\t\t\t\tcv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t# Showing the frame\n",
    "\t\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "\t# Break from loop/end when the 'q' is pressed\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "        \n",
    "# Destroying the windows once the frame display is over\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n",
    "print(sum(mouthDistance)/len(mouthDistance))\n",
    "print(sum(ears)/len(ears))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitc2147c82098346c280246e5c84ab2d48",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}